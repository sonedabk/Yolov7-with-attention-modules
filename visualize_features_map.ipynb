{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["YOLOR ðŸš€ v0.1-152-gfae9169 torch 2.0.1+cu117 CPU\n","\n"]},{"name":"stdout","output_type":"stream","text":["Fusing layers... \n","IDetect.fuse\n"]},{"name":"stderr","output_type":"stream","text":["/home/edabk/anaconda3/envs/yolov7/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Model Summary: 343 layers, 6175567 parameters, 0 gradients, 13.3 GFLOPS\n"]},{"name":"stdout","output_type":"stream","text":[" Convert model to Traced-model... \n"]},{"name":"stderr","output_type":"stream","text":["\n","(eog:270492): EOG-CRITICAL **: 21:59:11.202: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n","\n","(eog:270492): GLib-GIO-CRITICAL **: 21:59:11.202: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n","\n","(eog:270492): EOG-CRITICAL **: 21:59:12.322: eog_image_get_file: assertion 'EOG_IS_IMAGE (img)' failed\n","\n","(eog:270492): GLib-GIO-CRITICAL **: 21:59:12.322: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n"]},{"name":"stdout","output_type":"stream","text":[" traced_script_module saved! \n"," model is traced! \n","\n","torch.Size([1, 3, 384, 640])\n","Done. (2.449s)\n"]}],"source":["import argparse\n","import time\n","from pathlib import Path\n","\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","\n","from models.experimental import attempt_load\n","from utils.datasets import LoadStreams, LoadImages\n","from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n","    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n","from models.common import CBAM\n","\n","\n","def detect(save_img=False):\n","    source, weights, view_img, save_txt, imgsz, trace = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace\n","    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n","    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n","        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n","    \n","\n","    # Directories\n","    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n","    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n","\n","    # Initialize\n","    set_logging()\n","    device = select_device(opt.device)\n","    half = device.type != 'cpu'  # half precision only supported on CUDA\n","\n","    # Load model\n","    model = attempt_load(weights, map_location=device)  # load FP32 model\n","    stride = int(model.stride.max())  # model stride\n","    imgsz = check_img_size(imgsz, s=stride)  # check img_size\n","\n","    if trace:\n","        model = TracedModel(model, device, opt.img_size)\n","\n","    if half:\n","        model.half()  # to FP16\n","\n","    # Second-stage classifier\n","    classify = False\n","    if classify:\n","        modelc = load_classifier(name='resnet101', n=2)  # initialize\n","        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n","\n","    # Set Dataloader\n","    vid_path, vid_writer = None, None\n","\n","    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n","\n","    # Get names and colors\n","    names = model.module.names if hasattr(model, 'module') else model.names\n","    colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n","\n","    # Run inference\n","    t0 = time.time()\n","    for path, img, im0s, vid_cap in dataset:\n","        img = torch.from_numpy(img).to(device)\n","        img = img.half() if half else img.float()  # uint8 to fp16/32\n","        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","        if img.ndimension() == 3:\n","            img = img.unsqueeze(0)\n","\n","        print(img.size())\n","        # Inference\n","        t1 = time_synchronized()\n","        with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n","            pred = model(img, augment=opt.augment)[0]\n","        t2 = time_synchronized()\n","\n","        \n","\n","    print(f'Done. ({time.time() - t0:.3f}s)')\n","\n","\n","if __name__ == '__main__':\n","\n","    \n","    class Params:\n","        weights = \"./runs/train/exp17/weights/best.pt\"\n","        source = './visualize_imgs/0000084_00000_d_0000001.jpg'\n","        img_size = 640\n","        conf_thres = 0.25\n","        iou_thres = 0.45\n","        device = 'cpu'\n","        view_img = None\n","        save_txt = None\n","        save_conf = None\n","        nosave = None\n","        classes = None\n","        agnostic_nms = None\n","        augment = None\n","        update = None\n","        project = \"\"\n","        name = \"\"\n","        exist_ok = \"\"\n","        no_trace = False\n","        \n","\n","\n","\n","    opt = Params()\n","\n","\n","\n","    detect()\n"]}],"metadata":{"kernelspec":{"display_name":"yolov7","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
